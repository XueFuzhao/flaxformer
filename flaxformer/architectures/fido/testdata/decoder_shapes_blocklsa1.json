{
  "decoder": {
    "layers_0": {
      "encoder_decoder_attention": {
        "key": {
          "kernel": [13, 2, 24]
        },
        "out": {
          "kernel": [24, 2, 13]
        },
        "query": {
          "kernel": [13, 2, 24]
        },
        "value": {
          "kernel": [13, 2, 24]
        }
      },
      "mlp": {
        "wi_0": {
          "kernel": [13, 2, 32]
        },
        "wi_1": {
          "kernel": [13, 2, 32]
        },
        "wo": {
          "kernel": [32, 2, 13]
        }
      },
      "pre_cross_attention_layer_norm": {
        "scale": [13, 2]
      },
      "pre_mlp_layer_norm": {
        "scale": [13, 2]
      },
      "pre_self_attention_layer_norm": {
        "scale": [13, 2]
      },
      "self_attention": {
        "key": {
          "kernel": [13, 2, 24]
        },
        "out": {
          "kernel": [24, 2, 13]
        },
        "query": {
          "kernel": [13, 2, 24]
        },
        "value": {
          "kernel": [13, 2, 24]
        }
      }
    }
  },
  "decoder_norm": {
    "scale": [13]
  },
  "token_embedder": {
    "embedding": [4, 13]
  }
}
